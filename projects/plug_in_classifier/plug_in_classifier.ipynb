{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plug-in classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are given a set of classes $\\mathcal{Y} = \\{0,1,...,K\\}$, a dataset $X = (x_1,...,x_n)$ and a set of ouputs $(y_1,...,y_n)$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathcal{Y}$. If we are given $P(Y=y)$(class prior) and $P(X=x|Y=y)$(data likelihood given class), then the Bayes classifier is given by\n",
    "\n",
    "$$f(x) = \\arg \\max_{y \\in \\mathcal{Y}} P(X=x | Y=y)P(Y=y).$$\n",
    "\n",
    "However, typically we do not know what the class prior and data likelihood are. In this case one can approximate $P(X=x|Y=y)$, $P(Y=y)$. One possible approach is to estimate the Bayes classifier via maximum likelihood estimate(MLE). The procedure is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One can check that the MLE of $P(y)$ is\n",
    "$$ \\hat{\\pi}_y = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}(y_i = y).$$\n",
    "\n",
    "2. Given $y$, assume that the data is generated from a gaussian distribution with density $N(x|\\mu_y, \\Sigma_y)$. Let\n",
    "$$ n_y = \\sum_{i=1}^n \\mathbb{1}(y_i=y).$$\n",
    "Then the MLE estimate of $(\\mu_y, \\Sigma_y)$ is:\n",
    "$$ \\hat{\\mu}_y = \\frac{1}{n_y} \\sum_{i=1}^n \\mathbb{1}(y_i = y) x_i,$$\n",
    "$$ \\hat{\\Sigma}_y = \\frac{1}{n_y} \\sum_{i=1}^n \\mathbb{1}(y_i = y) (x_i - \\hat{\\mu}_y)(x_i - \\hat{\\mu}_y)^T.$$\n",
    "\n",
    "3. The plug-in classifier is given by\n",
    "\n",
    "$$\\hat{f}(x) = \\arg \\max_{y \\in \\mathcal{Y}} \\hat{\\pi}_y |\\hat{\\Sigma}_y|^{-1/2} \n",
    "\\exp\\left( -\\frac{1}{2} (x-\\hat{\\mu}_y)^T \\hat{\\Sigma}_y^{-1} (x - \\hat{\\mu}_y) \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we implement and apply the plug-in classifier to Iris dataset that can be found at: https://archive.ics.uci.edu/ml/datasets/iris. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import, load and preprocess data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.5 2.3 1.3 0.3 0]\n",
      " [6.6 3.0 4.4 1.4 1]\n",
      " [6.0 3.4 4.5 1.6 1]\n",
      " [6.8 3.0 5.5 2.1 2]\n",
      " [6.3 2.8 5.1 1.5 2]\n",
      " [6.1 3.0 4.6 1.4 1]\n",
      " [4.9 2.4 3.3 1.0 1]\n",
      " [6.3 3.4 5.6 2.4 2]\n",
      " [4.4 2.9 1.4 0.2 0]]\n",
      "(150, 5)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## data preprocessing\n",
    "data = pd.read_csv('iris.csv', sep=\",\", header=None)\n",
    "\n",
    "for n in range(data.shape[0]):\n",
    "    if data.get_value(n, 4) == 'Iris-setosa':\n",
    "        data.set_value(n, 4, 0)\n",
    "    if data.get_value(n, 4) == 'Iris-versicolor':\n",
    "        data.set_value(n, 4, 1)\n",
    "    if data.get_value(n, 4) == 'Iris-virginica':\n",
    "        data.set_value(n, 4, 2)\n",
    "\n",
    "data = data.as_matrix()\n",
    "np.random.shuffle(data)\n",
    "print(data[1:10])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row corresponds to one iris (flower), the first four columns are certains parameters of the flower, for details see the link above. The last column is the type of the iris $\\in \\{0, 1, 2\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the implementation of the plug-in classifier we discussed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluginClassifier(X_train, y_train, X_test, classes=3):\n",
    "    rows_test = X_test.shape[0]\n",
    "    rows = X_train.shape[0]\n",
    "    columns = X_train.shape[1] # aka number of classes\n",
    "    proby = np.zeros((rows_test, classes)) # probabilities\n",
    "    piy= np.zeros(classes) # mle for prior\n",
    "    ny = np.zeros(classes) # number of elements in class\n",
    "    muy = np.zeros((classes, columns)) # mle for average\n",
    "    sigmay = np.zeros((columns, columns, classes)) # mle for covariance\n",
    "    invy = np.zeros((columns, columns, classes)) # inverses of sigmay\n",
    "    sqrty = np.zeros(classes) # sqrt of determinant of invy\n",
    "\n",
    "    ## compute mle(maximum likelihood estimate) for prior and average\n",
    "    for n in range(classes):\n",
    "        sumx = np.zeros(X_train.shape[1])\n",
    "        for k in range(rows):\n",
    "            if y_train[k] == n: # or y_train[k] == n+1 depending on indexing\n",
    "                ny[n] += 1\n",
    "                sumx = sumx + X_train[k, :]\n",
    "        piy[n] = ny[n]/rows\n",
    "        muy[n, :] = (1/ny[n])*sumx\n",
    "\n",
    "\n",
    "    ## compute mle for covariance\n",
    "    for n in range(classes):\n",
    "        for k in range(rows):\n",
    "            if y_train[k] == n:\n",
    "                aux = X_train[k, :] - muy[n]\n",
    "                sigmay[:, :, n] = sigmay[:, :, n] + np.outer(aux, aux)\n",
    "        sigmay[:, :, n] = (1/ny[n])*sigmay[:, :, n]\n",
    "        invy[:, :, n] = np.linalg.inv(sigmay[:, :, n])\n",
    "        sqrty[n] = np.sqrt(np.absolute(np.linalg.det(invy[:, :, n])))\n",
    "\n",
    "    ## plug-in classifier: computing not rescale p(x, y)\n",
    "    for k in range(rows_test):\n",
    "        for n in range(classes):\n",
    "            vec = X_test[k, :] - muy[n, :]\n",
    "            aux = (-0.5)*np.dot(vec.T, np.dot(invy[:, :, n], vec))\n",
    "            proby[k, n] = piy[n]*sqrty[n]*np.exp(aux)\n",
    "\n",
    "    aux = np.sum(proby, axis=1)\n",
    "\n",
    "    ## rescaling probabilities p(x,y)\n",
    "    for k in range(rows_test):\n",
    "        for n in range(classes):\n",
    "            proby[k, n] = proby[k, n]/aux[k]\n",
    "\n",
    "    return proby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function return a matrix of (normalized) probabilities, where proby(i,k) is the probability that that the i-th element of the test set is in class k. Knowing these probabilities we can predict the classes. \n",
    "\n",
    "Since we are not tweaking any hyperparameters here, I tested the performance of the algorithm shuffling the dataset many times, splitting it into train and test set, checking the test accuracy and finally averaging over the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.974666666667\n"
     ]
    }
   ],
   "source": [
    "## classifying according to output probabilities several times for shuffled data\n",
    "test_acc = 0\n",
    "iterations = 100\n",
    "for l in range(iterations):\n",
    "    # print(l+1)\n",
    "    np.random.shuffle(data)\n",
    "    X_train = data[0:120, 0:4]\n",
    "    y_train = data[0:120, 4]\n",
    "    X_test = data[120:150, 0:4]\n",
    "    y_test = data[120:150, 4]\n",
    "    probabilities = pluginClassifier(X_train, y_train, X_test)\n",
    "    rows_test = X_test.shape[0]\n",
    "    y_out = np.zeros(rows_test)\n",
    "    ## classifying according to the highest probability\n",
    "    for k in range(rows_test):\n",
    "        vec = probabilities[k, :]\n",
    "        y_out[k] = np.argmax(vec)\n",
    "        test_acc += np.sum(y_out[k]==y_test[k] )/30\n",
    "\n",
    "print(\"Average accuracy: \", test_acc/iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
